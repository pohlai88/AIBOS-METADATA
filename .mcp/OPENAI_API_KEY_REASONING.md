# Why OpenAI API Key is Required for UI Generator MCP

> **Date:** 2025-11-24  
> **Purpose:** Explain why `aibos-ui-generator` requires OpenAI API key instead of using Cursor's local LLM

---

## ğŸ¤” Question

**"Since we are using Cursor during development stage, why does it require the OpenAI API key? Why not utilize the generative AI in Cursor / local LLM where available?"**

---

## ğŸ“‹ Answer

### **Short Answer**

The `aibos-ui-generator` MCP server requires an OpenAI API key because:

1. **MCP Servers Run Independently** - MCP servers are separate processes that don't have direct access to Cursor's LLM
2. **Explicit Control** - Using OpenAI API gives us explicit control over model selection, temperature, and system prompts
3. **Consistency** - Ensures consistent generation behavior regardless of Cursor's LLM configuration
4. **Specialized Models** - Allows us to use specialized models (e.g., `gpt-5.1-thinking`) optimized for code generation
5. **System Prompt Control** - Our system prompt (`.mcp/ui-generator/prompt.md`) is specifically tuned for OpenAI's API format

---

## ğŸ” Detailed Explanation

### 1. **MCP Architecture Separation**

**MCP servers are separate processes:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cursor IDE    â”‚
â”‚  (Local LLM)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ MCP Protocol (stdio)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Server     â”‚
â”‚  (Node.js)      â”‚  â† Separate process, no access to Cursor's LLM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- MCP servers run as **independent Node.js processes**
- They communicate with Cursor via **stdio (standard input/output)**
- MCP servers **cannot access Cursor's internal LLM** directly
- Each MCP server must provide its own LLM integration if needed

### 2. **Why Not Use Cursor's LLM?**

**Technical Limitations:**

1. **No Direct Access** - MCP protocol doesn't provide a way for servers to call Cursor's LLM
2. **Protocol Limitation** - MCP is designed for tools, not LLM access
3. **Process Isolation** - MCP servers are isolated processes for security

**Design Philosophy:**

1. **Explicit Dependencies** - MCP servers should be self-contained
2. **Portability** - Servers should work with any MCP client, not just Cursor
3. **Control** - Each server controls its own LLM configuration

### 3. **Benefits of OpenAI API**

#### **A. Model Selection**

We can choose models optimized for code generation:

```javascript
const DEFAULT_MODEL = 
  process.env.AIBOS_UI_GENERATOR_MODEL || "gpt-5.1-thinking";
```

- **Specialized Models** - Use models tuned for code generation
- **Version Control** - Pin specific model versions
- **Easy Updates** - Switch models via environment variable

#### **B. System Prompt Control**

Our system prompt is specifically designed for OpenAI's format:

```javascript
const systemPrompt = loadUiGeneratorSystemPrompt();
// Loaded from .mcp/ui-generator/prompt.md
// Tuned for OpenAI's chat completion API
```

- **Custom Instructions** - Full control over system prompt
- **Version Controlled** - System prompt in `.mcp/ui-generator/prompt.md`
- **Optimized** - Tuned for OpenAI's API format

#### **C. Temperature & Parameters**

We can fine-tune generation parameters:

```javascript
const response = await openai.chat.completions.create({
  model: DEFAULT_MODEL,
  messages,
  temperature: 0.2, // Low temperature for consistent output
});
```

- **Consistent Output** - Low temperature (0.2) for deterministic results
- **Reproducible** - Same prompt = same output
- **Configurable** - Adjust parameters per use case

#### **D. Governance & Metadata**

We can track usage and enforce policies:

```javascript
const payload = {
  ok: true,
  result: uiOutput,
  format,
  mode,
  registryContext: GOVERNANCE_CONTEXT, // Track usage
};
```

- **Audit Trail** - Track all generations
- **Compliance** - Meet regulatory requirements
- **Analytics** - Monitor usage patterns

### 4. **Alternative Approaches**

#### **Option A: Use Cursor's LLM (Not Possible)**

âŒ **Not Supported by MCP Protocol**
- MCP doesn't provide LLM access to servers
- Would require protocol changes
- Breaks portability

#### **Option B: Local LLM Integration**

âœ… **Possible but Complex**

We could integrate local LLMs (e.g., Ollama, LM Studio):

```javascript
// Hypothetical local LLM integration
import { createOllama } from "@ai-sdk/ollama";

const ollama = createOllama({
  baseURL: "http://localhost:11434",
});

const response = await ollama.chat.completions.create({
  model: "llama3",
  messages,
});
```

**Challenges:**
- âŒ Model quality may vary
- âŒ Requires local LLM setup
- âŒ System prompt may need retuning
- âŒ Performance may be slower
- âŒ Consistency may vary

**When to Use:**
- âœ… Privacy-sensitive environments
- âœ… Offline development
- âœ… Cost optimization

#### **Option C: Hybrid Approach (Future)**

âœ… **Best of Both Worlds**

We could support both OpenAI and local LLMs:

```javascript
const useLocalLLM = process.env.USE_LOCAL_LLM === "true";
const llm = useLocalLLM 
  ? createOllama({ baseURL: "http://localhost:11434" })
  : createOpenAI({ apiKey: process.env.OPENAI_API_KEY });
```

**Benefits:**
- âœ… Flexibility - Use either option
- âœ… Fallback - Local LLM if API unavailable
- âœ… Privacy - Local LLM for sensitive code

---

## ğŸ¯ Current Implementation

### **Why OpenAI API is Required**

1. **MCP Protocol Limitation** - Servers can't access Cursor's LLM
2. **Explicit Control** - Full control over model and parameters
3. **Consistency** - Reproducible, consistent output
4. **System Prompt** - Optimized for OpenAI's API format
5. **Governance** - Track usage and enforce policies

### **When OpenAI API is NOT Required**

The following MCP servers **do NOT** require OpenAI API:

- âœ… `aibos-filesystem` - File operations only
- âœ… `react-validation` - AST-based validation
- âœ… `aibos-theme` - Token management
- âœ… `aibos-documentation` - Documentation automation
- âœ… `aibos-component-generator` - Rule-based generation (no LLM)
- âœ… `aibos-a11y-validation` - Accessibility validation

**Only `aibos-ui-generator` requires OpenAI API** because it generates code from natural language.

---

## ğŸ”® Future Enhancements

### **1. Local LLM Support**

Add support for local LLMs (Ollama, LM Studio):

```javascript
// .mcp/ui-generator/server.mjs
const LLM_PROVIDER = process.env.LLM_PROVIDER || "openai";

if (LLM_PROVIDER === "ollama") {
  const ollama = createOllama({
    baseURL: process.env.OLLAMA_BASE_URL || "http://localhost:11434",
  });
  // Use Ollama
} else {
  const openai = createOpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });
  // Use OpenAI
}
```

### **2. Cursor LLM Integration (If Protocol Supports)**

If MCP protocol adds LLM access:

```javascript
// Hypothetical future API
const response = await mcpClient.generate({
  model: "cursor-llm",
  messages,
  systemPrompt: loadUiGeneratorSystemPrompt(),
});
```

### **3. Hybrid Mode**

Support both OpenAI and local LLM with fallback:

```javascript
async function generateWithFallback(messages) {
  try {
    return await openai.chat.completions.create({ messages });
  } catch (error) {
    if (process.env.FALLBACK_TO_LOCAL === "true") {
      return await ollama.chat.completions.create({ messages });
    }
    throw error;
  }
}
```

---

## ğŸ“ Summary

**Why OpenAI API Key is Required:**

1. âœ… **MCP servers are separate processes** - No access to Cursor's LLM
2. âœ… **Explicit control** - Full control over model and parameters
3. âœ… **Consistency** - Reproducible, consistent output
4. âœ… **System prompt** - Optimized for OpenAI's API format
5. âœ… **Governance** - Track usage and enforce policies

**Future Options:**

- ğŸ”® Local LLM support (Ollama, LM Studio)
- ğŸ”® Hybrid mode (OpenAI + local fallback)
- ğŸ”® Cursor LLM integration (if protocol supports)

**Current Status:**

- âœ… `aibos-ui-generator` requires OpenAI API key
- âœ… All other MCP servers work without OpenAI API
- âœ… OpenAI API key is optional (server works without it, but generation tools won't function)

---

**Last Updated:** 2025-11-24  
**Maintained By:** AI-BOS MCP Team

